{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etapa 1: Importação das bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install bert-for-tf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%tensorflow_version 2.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "import bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etapa 2: Carregamento da base de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "cols = ['sentiment', 'id', 'date', 'query', 'user', 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('/content/drive/MyDrive/ProjetoIA/training.1600000.processed.noemoticon.csv',\n",
    "                   header = None,\n",
    "                   names = cols,\n",
    "                   engine = 'python',\n",
    "                   encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "data.drop(['id', 'date', 'query', 'user'],\n",
    "          axis=1,\n",
    "          inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etapa 3: Limpeza dos textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def limpar_tweet(tweet):\n",
    "  tweet = BeautifulSoup(tweet, 'lxml').get_text() #Deixar o texto em um formato adequado para fazer a limpeza\n",
    "  tweet = re.sub(r\"@[A-Za-z0-9]+\", ' ', tweet)\n",
    "  tweet = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', tweet)\n",
    "  tweet = re.sub(r\"[^a-zA-Z.!?']\", ' ', tweet)\n",
    "  tweet = re.sub(r\" +\", ' ', tweet)\n",
    "\n",
    "  return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "teste = data['text'][0]\n",
    "teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "result = limpar_tweet(teste)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "data_limpo = [limpar_tweet(tweet) for tweet in data.text] #Utiliza a função para limpar todos os tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "data_limpo[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "data_labels = data.sentiment.values #Ajustando os valores do sentimento positivo\n",
    "data_labels[data_labels == 4] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etapa 4: Tokenização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "FullTokenizer = bert.bert_tokenization.FullTokenizer\n",
    "bert_layer = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1', trainable=False) #Pegar modelo\n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy() #Adicionando o arquivo de vocabulario\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy() #Deixando transformar em minusculo\n",
    "tokenizer = FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer.tokenize(\"My cat is happy.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def encode_sentence(sent):\n",
    "  return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "encode_sentence(\"My dog is hungry.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "data_entradas = [encode_sentence(sentence) for sentence in data_limpo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "data_entradas[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etapa 5: Criação da base de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "data_with_len = [[sent, data_labels[i], len(sent)]\n",
    "                 for i, sent in enumerate(data_entradas)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "data_with_len[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "random.shuffle(data_with_len) #Mistura os dados da base de dados\n",
    "data_with_len.sort(key=lambda x: x[2]) #Ordenada de forma crescente pegando o tamanho\n",
    "sorted_all = [(sent_lab[0], sent_lab[1])\n",
    "              for sent_lab in data_with_len if sent_lab[2] > 7] #Cria uma nova lista e pega a textos mais relevantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "all_dataset = tf.data.Dataset.from_generator(lambda: sorted_all,\n",
    "                                             output_types=(tf.int32, tf.int32)) #Tranformar os dados para que o tensorflow consiga interpretar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "all_batched = all_dataset.padded_batch(BATCH_SIZE, padded_shapes=((None,), ())) #Definir o numero de registros em cada batch e criar uma lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "next(iter(all_batched))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "NB_BATCHES = len(sorted_all) // BATCH_SIZE #Quantidade de batches para treinamento\n",
    "NB_BATCHES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "NB_BATCHES_TEST = NB_BATCHES // 10 #Quantidade de batches para teste\n",
    "NB_BATCHES_TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "all_batched.shuffle(NB_BATCHES) #Embaralhar os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "test_dataset = all_batched.take(NB_BATCHES_TEST) #Criar base de dados de teste\n",
    "train_dataset = all_batched.skip(NB_BATCHES_TEST) #Crirar base de dados de treinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etapa 6: Criação do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class DCNN(tf.keras.Model):\n",
    "\n",
    "  def __init__(self,\n",
    "               vocab_size,\n",
    "               emb_dim = 128,\n",
    "               nb_filters = 50,\n",
    "               FFN_units=512,\n",
    "               nb_classes = 2,\n",
    "               dropout_rate=0.1,\n",
    "               training = False,\n",
    "               name=\"dcnn\"):\n",
    "    super(DCNN, self).__init__(name=name)\n",
    "\n",
    "    self.embedding = layers.Embedding(vocab_size, emb_dim) #Criando embedding com o tamanho do voabulario x dimensão embedding\n",
    "\n",
    "    #Definição dos filtros\n",
    "    self.bigram = layers.Conv1D(filters = nb_filters,\n",
    "                                kernel_size = 2,\n",
    "                                padding = 'valid',\n",
    "                                activation = 'relu')\n",
    "\n",
    "    self.trigram = layers.Conv1D(filters = nb_filters,\n",
    "                                kernel_size = 3,\n",
    "                                padding = 'valid',\n",
    "                                activation = 'relu')\n",
    "    self.fourgram = layers.Conv1D(filters = nb_filters,\n",
    "                                kernel_size = 4,\n",
    "                                padding = 'valid',\n",
    "                                activation = 'relu')\n",
    "\n",
    "    self.pool = layers.GlobalMaxPool1D() #Camada de pooling\n",
    "\n",
    "    self.dense_1 = layers.Dense(units = FFN_units, activation = 'relu') #Camada mais densa\n",
    "\n",
    "    self.dropout = layers.Dropout(rate = dropout_rate) #Camada de dropout\n",
    "\n",
    "    if nb_classes == 2:\n",
    "      self.last_dense = layers.Dense(units = 1, activation = 'sigmoid')\n",
    "    else:\n",
    "      self.leat_dense = layers.Dense(units = nb_classes, activation = 'softmax')\n",
    "\n",
    "  def call(self, inputs, training):\n",
    "    x = self.embedding(inputs)\n",
    "    x_1 = self.bigram(x)\n",
    "    x_1 = self.pool(x_1)\n",
    "    x_2 = self.trigram(x)\n",
    "    x_2 = self.pool(x_2)\n",
    "    x_3 = self.fourgram(x)\n",
    "    x_3 = self.pool(x_3)\n",
    "\n",
    "    merged = tf.concat([x_1, x_2, x_3], axis = -1)\n",
    "    merged = self.dense_1(merged)\n",
    "    merged = self.dropout(merged, training)\n",
    "    output = self.last_dense(merged)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etapa 7: Treinamento do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "VOCABULARIO_SIZE = len(tokenizer.vocab)\n",
    "EMB_DIM = 200\n",
    "NB_FILTERS = 100\n",
    "FFN_UNITS = 256 #Número de neuronios na camada densa\n",
    "NB_CLASSES = 2\n",
    "DROPOUT_RATE = 0.2 #Zerar neuronios\n",
    "NB_EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "Dcnn = DCNN(vocab_size=VOCABULARIO_SIZE,\n",
    "            emb_dim=EMB_DIM,\n",
    "            nb_filters = NB_FILTERS,\n",
    "            FFN_units=FFN_UNITS,\n",
    "            nb_classes = NB_CLASSES,\n",
    "            dropout_rate = DROPOUT_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if NB_CLASSES == 2:\n",
    "  Dcnn.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "else:\n",
    "  Dcnn.compile(loss = 'sparce_categorical_crossentropy', optimizer = 'adam', metrics = ['sparce_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"/content/drive/MyDrive/ProjetoIA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "cktp = tf.train.Checkpoint(Dcnn=Dcnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "cktp_manager = tf.train.CheckpointManager(cktp, checkpoint_path, max_to_keep=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if cktp_manager.latest_checkpoint:\n",
    "  cktp.restore(cktp_manager.latest_checkpoint)\n",
    "  print('Ultimo checkpoint resturado!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class MyCustomCallBack(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "    cktp_manager.save()\n",
    "    print(\"Checkpoint salvo!\".format(checkpoint_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "history = Dcnn.fit(train_dataset,\n",
    "                   epochs = NB_EPOCHS,\n",
    "                   steps_per_epoch = 100,\n",
    "                   callbacks=[MyCustomCallBack()]) #Treinamento do modelo"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
